# ДЗ по "Сбор и разметка данных (семинары)"

## Семинар 1. Основы клиент-серверного взаимодействия. Парсинг API
* Домашнее задание:
1.	Ознакомиться с некоторые интересными API. 
https://docs.ozon.ru/api/seller/ https://developers.google.com/youtube/v3/getting-started https://spoonacular.com/food-api
2.	Потренируйтесь делать запросы к API. Выберите публичный API, который вас интересует, и потренируйтесь делать 
API-запросы с помощью Postman. Поэкспериментируйте с различными типами запросов и попробуйте получить различные 
типы данных.
3.	Сценарий Foursquare
4.	Напишите сценарий на языке Python, который предложит пользователю ввести интересующую его категорию (например, 
кофейни, музеи, парки и т.д.).
5.	Используйте API Foursquare для поиска заведений в указанной категории.
6.	Получите название заведения, его адрес и рейтинг для каждого из них.
7.	Скрипт должен вывести название и адрес и рейтинг каждого заведения в консоль.

## Семинар 2. Парсинг HTML. BeautifulSoup
* Домашнее задание:  
Выполнить скрейпинг данных в веб-сайта http://books.toscrape.com/ и извлечь информацию о всех книгах на сайте во 
всех категориях: название, цену, количество товара в наличии (In stock (19 available)) в формате integer, описание.
Затем сохранить эту информацию в JSON-файле

## Семинар 3. Системы управления базами данных MongoDB и Кликхаус в Python
* Домашнее задание:
1.	Установите MongoDB на локальной машине, а также зарегистрируйтесь в онлайн-сервисе. https://www.mongodb.com/ https://www.mongodb.com/products/compass
2.	Загрузите данные который вы получили на предыдущем уроке путем скрейпинга сайта с помощью Buautiful Soup в MongoDB и создайте базу данных и коллекции для их хранения.
3.	Поэкспериментируйте с различными методами запросов.
4.	Зарегистрируйтесь в ClickHouse.
5.	Загрузите данные в ClickHouse и создайте таблицу для их хранения.

## Семинар 4. Парсинг HTML. XPathПарсинг HTML. XPath
* Домашнее задание:  
- Выберите веб-сайт с табличными данными, который вас интересует.
Напишите код Python, использующий библиотеку requests для отправки HTTP GET-запроса на сайт и получения 
HTML-содержимого страницы.
Выполните парсинг содержимого HTML с помощью библиотеки lxml, чтобы извлечь данные из таблицы.
Сохраните извлеченные данные в CSV-файл с помощью модуля csv.  

- Ваш код должен включать следующее:  
Строку агента пользователя в заголовке HTTP-запроса, чтобы имитировать веб-браузер и избежать блокировки сервером.
Выражения XPath для выбора элементов данных таблицы и извлечения их содержимого.
Обработка ошибок для случаев, когда данные не имеют ожидаемого формата.
Комментарии для объяснения цели и логики кода.  
  
- Примечание: Пожалуйста, не забывайте соблюдать этические и юридические нормы при веб-скреппинге.

## Семинар 5. Scrapy
* Домашнее задание:  
1.	Найдите сайт, содержащий интересующий вас список или каталог. Это может быть список книг, фильмов, спортивных команд или что-то еще, что вас заинтересовало.
2.	Создайте новый проект Scrapy и определите нового паука. С помощью атрибута start_urls укажите URL выбранной вами веб-страницы.
3.	Определите метод парсинга для извлечения интересующих вас данных. Используйте селекторы XPath или CSS для навигации по HTML и извлечения данных. Возможно, потребуется извлечь данные с нескольких страниц или перейти по ссылкам на другие страницы.
4.	Сохраните извлеченные данные в структурированном формате. Вы можете использовать оператор yield для возврата данных из паука, которые Scrapy может записать в файл в выбранном вами формате (например, JSON или CSV).
5.	Конечным результатом работы должен быть код Scrapy Spider, а также пример выходных данных. Не забывайте соблюдать правила robots.txt и условия обслуживания веб-сайта, а также ответственно подходите к использованию веб-скрейпинга.
